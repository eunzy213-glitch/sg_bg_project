# pipeline.py
# ============================================================
# SG â†’ BG ì˜ˆì¸¡ ì „ì²´ íŒŒì´í”„ë¼ì¸
# - ì „ì²˜ë¦¬
# - ëª¨ë¸ í•™ìŠµ
# - í‰ê°€
# - ì‹œê°í™”
# - ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ (Streamlit/ì¶”ë¡ ìš©)
# - K-Fold êµì°¨ê²€ì¦ ì‹œê°í™”
# - ìµœì¢… ì¶”ë¡  ëª¨ë¸(pkl) ì €ì¥
# ============================================================

import os # íŒŒì¼/í´ë” ê²½ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import shutil # í´ë” ì‹œìŠ¤í…œ ì‘ì—… ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd # DataFrame ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
import joblib # í•™ìŠµëœ ëª¨ë¸ .pkl í˜•íƒœë¡œ ì €ì¥/ë¡œë“œí•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import logging # âœ… ë¡œê·¸ ì¶œë ¥ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì¶”ê°€)
import numpy as np # ìˆ˜ì¹˜ì—°ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
from pathlib import Path


from src.feature_builder import build_features # ëª¨ë¸ì— ë„£ì„ x, y, feature_name ìƒì„± 
from src.preprocessing import preprocess_and_filter_outliers # ì „ì²˜ë¦¬ ì „ì²´ ë¡œì§ ì²˜ë¦¬
from src.models import get_model_dict, train_and_predict_all  # ëª¨ë¸ë“¤ì„ dict í˜•íƒœë¡œ ë°˜í™˜, í•™ìŠµ ë° ì˜ˆì¸¡ ìˆ˜í–‰ 
from src.evaluation import (evaluate_all_models_overall, kfold_evaluate_models) # ëª¨ë¸ë³„ ì„±ëŠ¥ì§€í‘œ ë°˜í™˜, k-fold êµì°¨ê²€ì¦ ìˆ˜í–‰
from src.visualization import (plot_scatter, plot_actual_vs_pred, plot_residual, plot_bland_altman, plot_cega, plot_model_metrics) # ì‹œê°í™” í•¨ìˆ˜ë“¤
from src.evaluation_with_seg import (evaluate_model_with_seg, create_combined_summary, compare_seg_across_models) # âœ… SEG ë¶„ì„ ê´€ë ¨ í•¨ìˆ˜ë“¤ (ì¶”ê°€)

# --------------------------------------------------------
# âœ… Logger ì„¤ì • (ì¶”ê°€)
# --------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s"
)
logger = logging.getLogger(__name__)


def run_pipeline(data_path, experiment_name, feature_mode):
    """
    í•˜ë‚˜ì˜ ì‹¤í—˜(SG_ONLY / SG_PLUS_META)ì„
    ì²˜ìŒë¶€í„° ëê¹Œì§€ ì‹¤í–‰í•˜ëŠ” íŒŒì´í”„ë¼ì¸
    """

    logger.info(f"ğŸš€ Pipeline ì‹œì‘ | experiment={experiment_name}, feature_mode={feature_mode}")

    try:  # âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ë³´í˜¸ (ì¶”ê°€)

        # --------------------------------------------------------
        # 0ï¸âƒ£ ê²°ê³¼ í´ë” ì´ˆê¸°í™”
        # --------------------------------------------------------
        results_dir = os.path.join("results", experiment_name) # ê²°ê³¼ë¥¼ ì €ì¥í•  í´ë” ìƒì„±

        if os.path.exists(results_dir): # ì¬ì‹¤í–‰ ì‹œ ê¸°ì¡´ ê²°ê³¼ ì‚­ì œ
            logger.warning(f"âš ï¸ ê¸°ì¡´ ê²°ê³¼ í´ë” ì‚­ì œ: {results_dir}")
            try:  # âœ… PermissionError ë°©ì–´ (ì¶”ê°€)
                shutil.rmtree(results_dir)
            except PermissionError as e:
                logger.error(f"âŒ ê²°ê³¼ í´ë” ì‚­ì œ ì‹¤íŒ¨ (ê¶Œí•œ ë¬¸ì œ): {e}")
                raise e

        os.makedirs(results_dir, exist_ok=True) # í´ë” ìƒì„±
        logger.info(f"ğŸ“ ê²°ê³¼ í´ë” ìƒì„± ì™„ë£Œ: {results_dir}")

        # --------------------------------------------------------
        # 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ
        # --------------------------------------------------------
        logger.info(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì‹œì‘: {data_path}")
        df = pd.read_csv(data_path) # ì›ë³¸ë°ì´í„° ë¡œë“œ
        logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ | rows={len(df)}, cols={len(df.columns)}")

        # --------------------------------------------------------
        # 2ï¸âƒ£ ì œì™¸ ì»¬ëŸ¼ ì œê±°
        # --------------------------------------------------------
        drop_cols = [c for c in ["Gender", "Target_R"] if c in df.columns] # Gender, Target_R ì»¬ëŸ¼ ì œê±°
        df = df.drop(columns=drop_cols) # drop_colsì— ë“¤ì–´ìˆëŠ” ì»¬ëŸ¼ ì œê±°
        logger.info(f"ğŸ§¹ ì œê±°ëœ ì»¬ëŸ¼: {drop_cols}")

        # --------------------------------------------------------
        # 3ï¸âƒ£ ì „ì²˜ë¦¬ + ì´ìƒì¹˜ ì œê±°
        # --------------------------------------------------------
        logger.info("ğŸ§ª ì „ì²˜ë¦¬ ë° ì´ìƒì¹˜ ì œê±° ì‹œì‘")
        df_clean, filter_report = preprocess_and_filter_outliers(df) # ì „ì²˜ë¦¬ ë° ì´ìƒì¹˜ ì œê±° ìˆ˜í–‰

        # index ì •í•©ì„± ìœ ì§€
        df_clean = df_clean.reset_index(drop=True) # ì¸ë±ìŠ¤ ì¬ì •ë ¬

        filter_report.to_csv( # ì „ì²˜ë¦¬/ì´ìƒì¹˜ ì œê±° ë¦¬í¬íŠ¸ csv ì €ì¥
            os.path.join(results_dir, "filter_report.csv"),
            index=False
        )

        logger.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ | before={len(df)}, after={len(df_clean)}")

        # --------------------------------------------------------
        # 4ï¸âƒ£ Feature êµ¬ì„± (ëª¨ë¸ì— ë„£ì„ X, y ìƒì„±)
        # --------------------------------------------------------
        logger.info("ğŸ§© Feature êµ¬ì„± ì‹œì‘")
        X, y, feature_names = build_features(
            df_clean,
            mode=feature_mode
        )
        logger.info(f"âœ… Feature êµ¬ì„± ì™„ë£Œ | X.shape={X.shape}, feature_count={len(feature_names)}")

        # --------------------------------------------------------
        # 5ï¸âƒ£ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ (train/test split ë‚´ë¶€ ì²˜ë¦¬)
        # --------------------------------------------------------
        models = get_model_dict() # ì‚¬ìš©í•  ëª¨ë¸ì„ dict í˜•íƒœë¡œ ë°›ì•„ì˜´
        logger.info(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸ ëª©ë¡: {list(models.keys())}")

        pred_pack = train_and_predict_all(X, y, models) # test setì— ëŒ€í•œ ì˜ˆì¸¡ê°’ ë°˜í™˜
        logger.info("âœ… ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì™„ë£Œ")

        # ========================================================
        # ğŸ”¥ 5ï¸âƒ£-1ï¸âƒ£ Weighted Ensemble (Hold-out Test ì •ì‹ í¸ì…)
        # ========================================================
        if experiment_name == "SG_PLUS_META":

            logger.info("ğŸ§© Weighted Ensemble Hold-out ì˜ˆì¸¡ ìƒì„± ì‹œì‘")

            ensemble_weights = {
                "Linear": 0.05,
                "Polynomial": 0.10,
                "Huber": 0.15,
                "RandomForest": 0.20,
                "LightGBM": 0.50
            }

            y_true = pred_pack["y_test"]
            ensemble_pred = np.zeros_like(y_true, dtype=float)

            for model_name, weight in ensemble_weights.items():
                ensemble_pred += weight * pred_pack["preds"][model_name]

            # âœ… ì •ì‹ ëª¨ë¸ë¡œ ë“±ë¡
            pred_pack["preds"]["WeightedEnsemble"] = ensemble_pred

            logger.info("âœ… Weighted Ensemble Hold-out ì˜ˆì¸¡ ì™„ë£Œ")

        # --------------------------------------------------------
        # 6ï¸âƒ£ ì„±ëŠ¥ í‰ê°€ (Hold-out Test)
        # --------------------------------------------------------
        logger.info("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
        overall_metrics = evaluate_all_models_overall(pred_pack) # ëª¨ë¸ë³„ ì„±ëŠ¥ì§€í‘œ ê³„ì‚°
        overall_metrics["experiment"] = experiment_name # ì‹¤í—˜ëª… ì»¬ëŸ¼ ì¶”ê°€

        overall_metrics.to_csv(
            os.path.join(results_dir, "overall_metrics.csv"),
            index=False
        )

        logger.info("âœ… ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ì €ì¥ ì™„ë£Œ")
        
        # --------------------------------------------------------
        # âœ… SEG ê¸°ë°˜ ì¶”ê°€ í‰ê°€ (Step2-2 ë°˜ì˜)
        # --------------------------------------------------------
        logger.info("ğŸ“Œ SEG ê¸°ë°˜ í‰ê°€ ì‹œì‘ (SEG Analysis)")

        all_results = {}  # âœ… ëª¨ë¸ë³„ SEG ê²°ê³¼ ì €ì¥ìš© dict

        for model_name, y_pred in pred_pack["preds"].items():

            logger.info(f"   â–¶ SEG í‰ê°€ ìˆ˜í–‰ ì¤‘: {model_name}")

            # âœ… ë°˜ë“œì‹œ ë¨¼ì € model_dir ìƒì„±í•´ì•¼ í•¨
            model_dir = os.path.join(results_dir, model_name)
            os.makedirs(model_dir, exist_ok=True)

            # âœ… SEG í¬í•¨ í‰ê°€ ì‹¤í–‰ (Pathë¡œ ë³€í™˜í•´ì„œ ì „ë‹¬)
            results = evaluate_model_with_seg(
                y_true=pred_pack["y_test"],
                y_pred=y_pred,
                model_name=model_name,
                results_dir=Path(model_dir),        # âœ… Path ë³€í™˜ ìœ ì§€
                experiment_name=experiment_name
            )

            # âœ… ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥
            all_results[model_name] = results

            # --------------------------------------------------------
            # âœ… ë¡œê·¸ ì¶œë ¥ (KeyError ë°©ì§€ ìˆ˜ì • ì™„ë£Œ)
            # --------------------------------------------------------
            metrics = results["metrics"]
            seg_stats = results["seg_results"]["statistics"]

            # âœ… SEG í†µê³„ key í™•ì¸ ì¶œë ¥
            logger.info(f"      ğŸ“Œ SEG statistics keys: {list(seg_stats.keys())}")

            # âœ… Acceptable_% keyê°€ ì—†ì„ ê²½ìš° ì•ˆì „ ì²˜ë¦¬
            acceptable = seg_stats.get("Acceptable_%", None)

            if acceptable is not None:
                logger.info(
                    f"      âœ… {model_name}: "
                    f"MARD={metrics['MARD']:.2f}%, "
                    f"SEG Acceptable={acceptable:.2f}%"
                )
            else:
                logger.info(
                    f"      âœ… {model_name}: "
                    f"MARD={metrics['MARD']:.2f}%, "
                    f"SEG Acceptable key not found"
                )

        # --------------------------------------------------------
        # âœ… í‰ê°€ ë£¨í”„ ì¢…ë£Œ í›„ ìš”ì•½ ìƒì„± (Step2-3 ë°˜ì˜)
        # --------------------------------------------------------
        logger.info("ğŸ“Œ SEG í†µí•© ìš”ì•½ íŒŒì¼ ìƒì„± ì‹œì‘")

        # âœ… í†µí•© ìš”ì•½ CSV ìƒì„± (experiment í´ë” ë£¨íŠ¸ì— ì €ì¥)
        summary_path = Path(results_dir) / "combined_summary_with_seg.csv"
        create_combined_summary(all_results, summary_path)

        # âœ… SEG ëª¨ë¸ ë¹„êµ Plot ìƒì„± (experiment í´ë” ë£¨íŠ¸ì— ì €ì¥)
        seg_comparison_path = Path(results_dir) / "seg_comparison_all_models.png"
        compare_seg_across_models(
            all_results,
            seg_comparison_path,
            experiment_name
        )

        logger.info("âœ… SEG ë¶„ì„ ì™„ë£Œ (ëª¨ë¸ë³„ í´ë”ì— ì €ì¥ë¨)")
        
        
        # --------------------------------------------------------
        # 7ï¸âƒ£ ì „ì²´ ë°ì´í„° ë¶„í¬ ì‹œê°í™”
        # --------------------------------------------------------
        logger.info("ğŸ“ˆ SG vs BG ì „ì²´ ì‚°ì ë„ ì‹œê°í™”")
        plot_scatter(df_clean, results_dir)

        y_true = pred_pack["y_test"]

        # --------------------------------------------------------
        # 8ï¸âƒ£ ëª¨ë¸ë³„ ì‹œê°í™” (ëª¨ë¸ë³„ í´ë”)
        # --------------------------------------------------------
        logger.info("ğŸ–¼ï¸ ëª¨ë¸ë³„ ì‹œê°í™” ìƒì„± ì‹œì‘")

        for model_name, y_pred in pred_pack["preds"].items():
            logger.info(f"   â–¶ ì‹œê°í™” ìƒì„± ì¤‘: {model_name}")

            model_dir = os.path.join(results_dir, model_name)
            os.makedirs(model_dir, exist_ok=True)

            plot_actual_vs_pred(y_true, y_pred, model_name, model_dir)
            plot_residual(y_true, y_pred, model_name, model_dir)
            plot_bland_altman(y_true, y_pred, model_name, model_dir)
            plot_cega(y_true, y_pred, model_name, model_dir)

        # --------------------------------------------------------
        # 9ï¸âƒ£ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ Bar Plot
        # --------------------------------------------------------
        logger.info("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ Bar Plot ìƒì„±")
        plot_model_metrics(overall_metrics, results_dir)

        # --------------------------------------------------------
        # ğŸ”Ÿ K-Fold êµì°¨ê²€ì¦ + ì‹œê°í™”
        # --------------------------------------------------------
        logger.info("ğŸ” K-Fold êµì°¨ê²€ì¦ ì‹œì‘")
        kfold_df = kfold_evaluate_models(df_clean, models)

        kfold_df.to_csv(
            os.path.join(results_dir, "kfold_metrics.csv"),
            index=False
        )

        logger.info("âœ… K-Fold ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

        # --------------------------------------------------------
        # 1ï¸âƒ£1ï¸âƒ£ ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ (Streamlit / ë¶„ì„ìš©)
        # --------------------------------------------------------
        logger.info("ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ CSV ìƒì„± ì‹œì‘")

        pred_rows = []
        test_idx = pred_pack["test_idx"]

        for model_name, y_pred in pred_pack["preds"].items():
            for i, idx in enumerate(test_idx):
                pred_rows.append({
                    "experiment": experiment_name,
                    "model": model_name,
                    "SG": df_clean.loc[idx, "SG"],
                    "y_true": y_true[i],
                    "y_pred": y_pred[i],
                    "residual": y_pred[i] - y_true[i],
                })

        pred_df = pd.DataFrame(pred_rows)

        pred_df.to_csv(
            os.path.join(results_dir, "predictions.csv"),
            index=False
        )

        logger.info("âœ… ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ")

        # --------------------------------------------------------
        # 1ï¸âƒ£2ï¸âƒ£ ìµœì  ëª¨ë¸ ì €ì¥ (ì¶”ë¡ ìš©)
        # --------------------------------------------------------
        if experiment_name == "SG_PLUS_META":

            logger.info("ğŸ† ìµœì¢… LightGBM ëª¨ë¸ ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ")

            lgbm_model = get_model_dict()["LightGBM"]
            lgbm_model.fit(X, y)

            model_save_path = os.path.join(
                "results",
                "SG_PLUS_META",
                "best_model_lightgbm.pkl"
            )

            joblib.dump(lgbm_model, model_save_path)

            logger.info(f"âœ… ì¶”ë¡ ìš© ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")

        logger.info(f"ğŸ‰ Pipeline ì¢…ë£Œ: {experiment_name}")

    except Exception as e:
        logger.exception("ğŸ”¥ Pipeline ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ")
        raise e
