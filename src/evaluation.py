# evaluation.py
# ============================================================
# ì´ íŒŒì¼ì€ "ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í–ˆëŠ”ê°€?"ë¥¼
# í†µê³„ì  + ì„ìƒì  ê´€ì ì—ì„œ í‰ê°€í•˜ëŠ” ëª¨ë“  ë¡œì§ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
#
# í¬í•¨ ë‚´ìš©:
# - íšŒê·€ ì„±ëŠ¥ ì§€í‘œ (R2, RMSE, MAE)
# - ì„ìƒ í•µì‹¬ ì§€í‘œ (MARD)
# - Clarke Error Grid Analysis (CEGA)
# - Blandâ€“Altman í†µê³„
# - K-Fold êµì°¨ê²€ì¦
# ============================================================

# ìˆ˜ì¹˜ ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import pandas as pd

# sklearnì˜ íšŒê·€ ì„±ëŠ¥ ì§€í‘œ í•¨ìˆ˜ë“¤
from sklearn.metrics import (
    r2_score,
    mean_squared_error,
    mean_absolute_error
)
from sklearn.model_selection import KFold # K-Fold êµì°¨ê²€ì¦ìš©
from sklearn.base import clone


# ------------------------------------------------------------
# RMSE ê³„ì‚° í•¨ìˆ˜
# ------------------------------------------------------------
def rmse(y_true, y_pred):
    """
    RMSE = sqrt(mean((y_true - y_pred)^2))
    â†’ ì˜¤ì°¨ í¬ê¸°ë¥¼ mg/dL ë‹¨ìœ„ë¡œ ì§ê´€ì ìœ¼ë¡œ í‘œí˜„
    """
    return np.sqrt(mean_squared_error(y_true, y_pred)) # np.sprtë¥¼ ì”Œì›Œ RMSEë¡œ ë³€í™˜


# ------------------------------------------------------------
# MARD ê³„ì‚° í•¨ìˆ˜
# ------------------------------------------------------------
def mard(y_true, y_pred, eps=1e-6):
    """
    MARD (Mean Absolute Relative Difference)
    = mean(|pred - true| / true) * 100

    í˜ˆë‹¹ ì„¼ì„œ/ì¶”ì • ëª¨ë¸ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì„ìƒ ì§€í‘œ
    """
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)

    return np.mean(np.abs(y_pred - y_true) / (np.abs(y_true) + eps)) * 100


# ------------------------------------------------------------
# Clarke Error Grid Analysis (CEGA)
# ------------------------------------------------------------
def clarke_error_grid(y_true, y_pred):
    """
    ê° ì˜ˆì¸¡ì„ Zone A~Eë¡œ ë¶„ë¥˜
    Zone A/B ë¹„ìœ¨ì´ ì„ìƒì ìœ¼ë¡œ ì¤‘ìš”
    """

    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)

    zones = np.array(["A"] * len(y_true), dtype=object)

    ratio = y_pred / np.clip(y_true, 1e-6, None)

    # Zone A (ì„ìƒì ìœ¼ë¡œ ì •í™•)
    zone_a = (
        ((y_true >= 70) & (np.abs(ratio - 1) <= 0.2)) |
        ((y_true < 70) & (np.abs(y_pred - y_true) <= 20))
    )
    zones[zone_a] = "A"

    # Zone E (ì™„ì „íˆ ë°˜ëŒ€ íŒë‹¨ â†’ ìœ„í—˜)
    zone_e = (
        ((y_true < 70) & (y_pred > 180)) |
        ((y_true > 180) & (y_pred < 70))
    )
    zones[zone_e] = "E"

    # Zone D (ì¹˜ë£Œ ëˆ„ë½ ìœ„í—˜)
    zone_d = (
        ((y_true > 240) & (70 <= y_pred) & (y_pred <= 180)) |
        ((y_true < 70) & (70 <= y_pred) & (y_pred <= 180))
    )
    zones[zone_d] = "D"

    # Zone C (ë¶ˆí•„ìš”í•œ ì¹˜ë£Œ ìœ„í—˜)
    zone_c = (
        ((70 <= y_true) & (y_true <= 180)) &
        ((y_pred < 70) | (y_pred > 240))
    )
    zones[zone_c] = "C"

    # ë‚˜ë¨¸ì§€ëŠ” Zone B
    zones[~(zone_a | zone_c | zone_d | zone_e)] = "B"

    return zones


# ------------------------------------------------------------
# CEGA ìš”ì•½ í†µê³„
# ------------------------------------------------------------
def cega_summary(y_true, y_pred):
    zones = clarke_error_grid(y_true, y_pred) # ê° ìƒ˜í”Œì˜ zone ê³„ì‚°

    return { # Zone A~E ë¹„ìœ¨ ê³„ì‚°
        f"CEGA_{z}": np.mean(zones == z) * 100
        for z in ["A", "B", "C", "D", "E"]
    }


# ------------------------------------------------------------
# Blandâ€“Altman í†µê³„
# ------------------------------------------------------------
def bland_altman(y_true, y_pred):
    diff = y_pred - y_true # ì˜ˆì¸¡ê°’ - ì‹¤ì œê°’
    mean_diff = np.mean(diff) # í‰ê·  ì°¨ì´
    std_diff = np.std(diff, ddof=1) # í‘œì¤€í¸ì°¨

    return {
        "BA_bias": mean_diff,
        "BA_LoA_low": mean_diff - 1.96 * std_diff,
        "BA_LoA_high": mean_diff + 1.96 * std_diff
    }


# ------------------------------------------------------------
# ë‹¨ì¼ ëª¨ë¸ í‰ê°€
# ------------------------------------------------------------
def evaluate_single(y_true, y_pred): # í•˜ë‚˜ì˜ ëª¨ë¸ì— ëŒ€í•´ ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ í•œë²ˆì— ê³„ì‚°
    metrics = {
        "r2": r2_score(y_true, y_pred),
        "rmse": rmse(y_true, y_pred),
        "mae": mean_absolute_error(y_true, y_pred),
        "mard": mard(y_true, y_pred)
    }

    metrics.update(cega_summary(y_true, y_pred)) # cega ìš”ì•½ í†µê³„ ì¶”ê°€
    metrics.update(bland_altman(y_true, y_pred)) # bland-altman í†µê³„ ì¶”ê°€

    return metrics


# ------------------------------------------------------------
# ì „ì²´ ëª¨ë¸ í‰ê°€
# ------------------------------------------------------------
def evaluate_all_models_overall(pred_pack):
    y_test = pred_pack["y_test"]
    preds = pred_pack["preds"]

    rows = []

    for model_name, y_pred in preds.items():
        result = evaluate_single(y_test, y_pred)
        result["model"] = model_name
        rows.append(result)

    return pd.DataFrame(rows)


# ------------------------------------------------------------
# K-Fold êµì°¨ê²€ì¦
# ------------------------------------------------------------
def kfold_evaluate_models(df, models, n_splits=5):
    X = df[["SG"]].values
    y = df["BG"].values

    kf = KFold(
        n_splits=n_splits,
        shuffle=True,
        random_state=42   # âœ… ì¬í˜„ì„± ê³ ì •
    )

    rows = []

    for model_name, model in models.items():
        for fold, (tr, te) in enumerate(kf.split(X), start=1):

            # ------------------------------------------------
            # âœ… Pipeline / ëª¨ë“  ëª¨ë¸ì— ì•ˆì „í•œ clone
            # ------------------------------------------------
            model_clone = clone(model)

            model_clone.fit(X[tr], y[tr])
            pred = model_clone.predict(X[te])

            result = evaluate_single(y[te], pred)
            result["model"] = model_name
            result["fold"] = fold

            rows.append(result)

    return pd.DataFrame(rows)


# ------------------------------------------------------------
# Weighted Ensemble (Hold-out / K-Fold ê³µìš©)
# ------------------------------------------------------------
def weighted_ensemble(preds_dict, weights):
    """
    preds_dict : {model_name: y_pred}
    weights    : {model_name: weight}
    """
    y_ens = np.zeros_like(next(iter(preds_dict.values())))
    for model, w in weights.items():
        y_ens += w * preds_dict[model]
    return y_ens


# ------------------------------------------------------------
# ğŸ†• K-Fold Weighted Ensemble êµì°¨ê²€ì¦
# ------------------------------------------------------------
def kfold_weighted_ensemble(df, models, weights, n_splits=5):
    """
    K-Fold í™˜ê²½ì—ì„œ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê°€ì¤‘ í‰ê· í•˜ì—¬
    Weighted Ensemble ì„±ëŠ¥ì„ í‰ê°€
    """

    X = df[["SG"]].values
    y = df["BG"].values

    kf = KFold(
        n_splits=n_splits,
        shuffle=True,
        random_state=42
    )

    rows = []

    for fold, (tr, te) in enumerate(kf.split(X), start=1):

        fold_preds = {}

        # --------------------------------------------
        # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡ ìˆ˜ì§‘
        # --------------------------------------------
        for model_name, model in models.items():
            model_clone = clone(model)
            model_clone.fit(X[tr], y[tr])
            fold_preds[model_name] = model_clone.predict(X[te])

        # --------------------------------------------
        # Weighted Ensemble ì˜ˆì¸¡
        # --------------------------------------------
        y_ens = weighted_ensemble(fold_preds, weights)

        result = evaluate_single(y[te], y_ens)
        result["model"] = "WeightedEnsemble"
        result["fold"] = fold

        rows.append(result)

    return pd.DataFrame(rows)
